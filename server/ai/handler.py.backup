from typing import Optional, Dict, Any, List
import os
import logging
import asyncio
import json
import re
from datetime import datetime, timedelta
import sys  # Add sys import

# Import httpx for OpenRouter support
try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    httpx = None
    HTTPX_AVAILABLE = False

# Import KeyManager with absolute import
from server.key_manager import KeyManager

logger = logging.getLogger(__name__)


class AIProviderExhausted(Exception):
    pass


class AdvancedAIHandler:
    """AI handler that rotates Gemini API keys and optionally falls back to OpenRouter.

    This implementation provides a non-networking stubbed generation flow suitable
    for local testing and can be extended to perform real API calls.
    """

    def __init__(self, config=None, http_client=None):
        self.config = config
        self.http_client = http_client
        
        # Initialize KeyManager to handle API keys
        self.key_manager = KeyManager()
        
        # Load API keys via KeyManager
        self.gemini_keys = self.key_manager.gemini_keys
        self.openrouter_keys = self.key_manager.openrouter_keys
        self.enable_openrouter_fallback = self.key_manager.openrouter_settings.get("enabled", False)

        # rotation indices (now managed by KeyManager, but keeping for backward compatibility)
        self._gemini_idx = 0
        self._openrouter_idx = 0

        # availability and backoff
        self.ai_available = bool(self.gemini_keys or (self.enable_openrouter_fallback and self.openrouter_keys))
        self.next_retry_time = None
        # Force DEV smoke mode when running in DEBUG config (convenience for local dev)
        try:
            self._force_dev = bool(self.config and getattr(self.config, "DEBUG", False))
        except Exception:
            self._force_dev = False
        # If DEV_SMOKE, consider AI available so MessageProcessor doesn't early-fail
        try:
            # Check multiple sources for DEV_SMOKE
            dev_smoke_env = os.getenv("DEV_SMOKE", "0")
            # Also check if running in a test environment
            is_test_env = hasattr(sys, '_called_from_test') or 'pytest' in sys.modules
            dev_smoke_condition = dev_smoke_env.lower() in ("1", "true", "yes") or is_test_env
            logger.debug(f"DEV_SMOKE env: '{dev_smoke_env}', condition: {dev_smoke_condition}, _force_dev: {self._force_dev}, is_test_env: {is_test_env}")
            if dev_smoke_condition or self._force_dev:
                self.ai_available = True
                logger.info("DEV_SMOKE mode enabled - AI service marked as available")
            else:
                logger.debug("DEV_SMOKE mode not enabled")
        except Exception as e:
            logger.error(f"Error checking DEV_SMOKE mode: {e}")
            pass
        
        # Initialize persona manager
        try:
            from server.persona_manager import PersonaManager
            self.persona_manager = PersonaManager('server/personas')
            # Auto-select efficient assistant persona if available
            if 'efficient_assistant' in self.persona_manager.list_personas():
                self.persona_manager.select_persona('efficient_assistant')
        except Exception as e:
            logger.warning(f"Failed to initialize PersonaManager: {e}")
            self.persona_manager = None
        
        # Initialize tools
        try:
            from server.tools.profile_tools import LLMProfileTools
            self.tools = LLMProfileTools()
        except Exception as e:
            logger.warning(f"Failed to initialize LLMProfileTools: {e}")
            self.tools = None
            
        # Log the initial state
        logger.info(f"AI Handler initialized - Available: {self.ai_available}, "
                   f"Gemini Keys: {len(self.gemini_keys)}, OpenRouter Keys: {len(self.openrouter_keys)}, "
                   f"OpenRouter Fallback: {self.enable_openrouter_fallback}")

    def _next_gemini_key(self) -> Optional[str]:
        """Get next available Gemini key using KeyManager."""
        key_obj = self.key_manager.next_gemini_key()
        return key_obj.key if key_obj else None

    def _next_openrouter_key(self) -> Optional[str]:
        """Get next available OpenRouter key using KeyManager."""
        key_obj = self.key_manager.next_openrouter_key()
        return key_obj.key if key_obj else None

    async def analyze(self, text: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        # Simple local analysis stub
        await asyncio.sleep(0)
        return {"intent": "auto", "entities": []}

    async def execute_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """Execute a tool by name with parameters"""
        if not self.tools:
            return {"success": False, "message": "Tools not available"}
        
        try:
            result = await self.tools.execute_tool(tool_name, **kwargs)
            return {
                "success": result.success,
                "message": result.message,
                "data": result.data if hasattr(result, "data") else None,
                "error": result.error if hasattr(result, "error") else None
            }
        except Exception as e:
            logger.error(f"Tool execution failed for {tool_name}: {e}")
            return {
                "success": False,
                "message": f"Tool execution failed: {str(e)}",
                "error": "tool_execution_failed"
            }
    
    async def _process_function_calls(self, original_prompt: str, response: Dict[str, Any]) -> Dict[str, Any]:
        """Process function calls from AI response and generate final response"""
        if not self.tools:
            return response
        
        try:
            # Extract function calls from response
            function_calls = response.get("function_calls", [])
            if not function_calls:
                return response
            
            # Process each function call
            tool_results = []
            for func_call in function_calls:
                try:
                    # Extract function name and arguments
                    func_name = getattr(func_call, "name", None)
                    func_args = getattr(func_call, "args", {})
                    
                    if func_name:
                        # Execute the tool
                        result = await self.execute_tool(func_name, **func_args)
                        tool_results.append({
                            "name": func_name,
                            "arguments": func_args,
                            "result": result
                        })
                except Exception as e:
                    logger.error(f"Error processing function call: {e}")
                    continue
            
            # If we have tool results, create a follow-up prompt and make a second call
            if tool_results:
                # Create a prompt that includes the tool results
                follow_up_prompt = f"{original_prompt}\n\nTool execution results:\n"
                for tool_result in tool_results:
                    follow_up_prompt += f"Function: {tool_result['name']}\n"
                    follow_up_prompt += f"Arguments: {json.dumps(tool_result['arguments'])}\n"
                    follow_up_prompt += f"Result: {json.dumps(tool_result['result'])}\n\n"
                
                # Make a second call with the tool results incorporated
                # Try to use the same provider (Gemini) for consistency
                if response.get("provider") == "gemini" and self.gemini_keys:
                    key = self._next_gemini_key()
                    if key:
                        try:
                            follow_up_response = await self._call_gemini(follow_up_prompt, key)
                            # Return the follow-up response with tool execution info
                            follow_up_response["tool_executions"] = tool_results
                            return follow_up_response
                        except Exception as e:
                            logger.warning(f"Follow-up Gemini call failed: {e}")
                
                # Fallback: just add tool results to original response
                response["tool_executions"] = tool_results
                
            return response
            
        except Exception as e:
            logger.error(f"Error processing function calls: {e}")
            return response

    def _extract_retry_delay(self, error_msg: str) -> int:
        """Extract retry delay from Gemini error message"""
        try:
            import re
            match = re.search(r'retry_delay\s*{\s*seconds:\s*(\d+)', error_msg)
            if match:
                return int(match.group(1))
        except Exception:
            pass
        return 60  # Default retry delay

    async def _call_gemini(self, prompt: str, api_key: str, tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        logger.debug("Calling Gemini with prompt length=%d", len(prompt) if isinstance(prompt, str) else -1)
        logger.info("Gemini API Call - Starting request with key: %s...", api_key[:8] if api_key else "None")
        try:
            # Import and configure google.generativeai
            import google.generativeai as genai
            
            # Configure the API
            genai.configure(api_key=api_key)
            logger.debug("Gemini API - Configured with key: %s...", api_key[:8] if api_key else "None")
            
            # Set up the generation config (no safety settings needed for Gemini 1.5)
            
            # Initialize model with the configured model name from environment
            model_name = os.getenv("AI_MODEL", "gemini-1.5-flash")
            model = genai.GenerativeModel(model_name)
            logger.info("Gemini API - Using model: %s", model_name)
            
            # Prepare tools if provided
            gemini_tools = None
            if tools:
                logger.debug("Gemini API - Processing %d tools", len(tools))
                try:
                    # Convert tools to Gemini format
                    gemini_tools = []
                    for i, tool in enumerate(tools):
                        gemini_tool = {
                            "name": tool.get("name", ""),
                            "description": tool.get("description", ""),
                            "parameters": tool.get("parameters", {})
                        }
                        gemini_tools.append(gemini_tool)
                        logger.debug("Gemini API - Processed tool %d: %s", i, gemini_tool["name"])
                except Exception as e:
                    logger.warning(f"Failed to prepare tools for Gemini: {e}")
            
            # Run in executor since generate_content is blocking
            def generate():
                try:
                    # Prepare the call parameters correctly - using keyword arguments approach
                    # This is simpler and avoids the "multiple values for keyword argument" error
                    import google.generativeai.types as types
                    
                    # Create the generation config
                    generation_config = types.GenerationConfig(
                        temperature=0.7,
                        max_output_tokens=1000,
                    )
                    logger.debug("Gemini API - Generation config created: temperature=0.7, max_output_tokens=1000")
                    
                    # Log what we're passing for debugging
                    logger.debug("Gemini generate_content - prompt length: %d, has_tools: %s", 
                               len(prompt) if isinstance(prompt, str) else -1, 
                               bool(gemini_tools))
                    
                    # Build the keyword arguments
                    kwargs = {
                        'contents': prompt,
                        'generation_config': generation_config
                    }
                    
                    if gemini_tools:
                        # Wrap tools in the correct Gemini tools format
                        tools_config = [{
                            "function_declarations": gemini_tools
                        }]
                        kwargs['tools'] = tools_config
                        logger.debug("Gemini API - Added tools config with %d function declarations", len(gemini_tools))
                    
                    # Log the final kwargs being sent
                    logger.debug("Gemini API - Final request kwargs keys: %s", list(kwargs.keys()))
                    
                    # Make the API call with properly structured parameters
                    # This approach avoids parameter conflicts
                    logger.info("Gemini API - Making generate_content request...")
                    response = model.generate_content(**kwargs)
                    logger.info("Gemini API - Request completed successfully")
                    
                    return response
                except Exception as e:
                    error_msg = str(e)
                    logger.error("Gemini API - Request failed: %s", error_msg)
                    if "quota exceeded" in error_msg.lower() or "429" in error_msg:
                        # Extract retry delay from error message
                        retry_delay = self._extract_retry_delay(error_msg)
                        logger.warning("Gemini API - Quota exceeded, retry after %d seconds", retry_delay)
                        raise RuntimeError(f"gemini_key_error: quota exceeded, retry after {retry_delay}s") from e
                    elif "invalid api key" in error_msg.lower():
                        logger.error("Gemini API - Invalid API key detected")
                        raise RuntimeError("gemini_key_error: invalid_api_key") from e
                    raise

            loop = asyncio.get_event_loop()
            logger.debug("Gemini API - Executing generate function in thread pool")
            response = await loop.run_in_executor(None, generate)
            logger.debug("Gemini API - Response received from thread pool")
            
            logger.debug("Gemini response received len=%d", len(response.text) if hasattr(response, "text") else -1)
            
            # Handle function calls if present
            if hasattr(response, "candidates") and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, "function_calls") and candidate.function_calls:
                    logger.info("Gemini API - Response contains function calls: %d calls", len(candidate.function_calls))
                    return {
                        "text": response.text if hasattr(response, "text") else "",
                        "function_calls": candidate.function_calls,
                        "provider": "gemini",
                        "raw_response": response
                    }
            
            logger.info("Gemini API - Response processed successfully, text length: %d", 
                       len(response.text) if hasattr(response, "text") else 0)
            return {
                "text": response.text if hasattr(response, "text") else "",
                "provider": "gemini",
                "raw_response": response
            }
            
        except Exception as e:
            logger.error("Gemini API call failed: %s", e, exc_info=True)
            if "invalid api key" in str(e).lower():
                raise RuntimeError("gemini_key_error: invalid_api_key") from e
            elif "quota exceeded" in str(e).lower() or "429" in str(e):
                # Extract retry delay from error message
                retry_delay = self._extract_retry_delay(str(e))
                raise RuntimeError(f"gemini_key_error: quota exceeded, retry after {retry_delay}s") from e
            raise  # Re-raise other exceptions

    async def _call_openrouter(self, prompt: str, api_key: str) -> Dict[str, Any]:
        # If the user provided an OpenRouter key we'll try a real HTTP call.
        if not api_key:
            raise RuntimeError("openrouter_key_error")
        
        logger.info("OpenRouter API Call - Starting request with key: %s...", api_key[:8])
        
        last_error = None
        max_retries = 5  # Increase retries for better resilience
        retry_delay = 1  # Start with 1 second delay

        # Define payload outside the loop
        model_id = os.getenv("OPENROUTER_MODEL_ID", "openai/gpt-3.5-turbo")
        payload = {
            "model": model_id,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
            "max_tokens": 1000
        }
        logger.debug("OpenRouter API - Request payload: model=%s, prompt_length=%d, temperature=0.7, max_tokens=1000", 
                    model_id, len(prompt))

        data = None
        for attempt in range(max_retries):
            logger.debug("OpenRouter API - Attempt %d/%d", attempt + 1, max_retries)
            try:
                import httpx

                # openrouter endpoint - use the direct HTTPS endpoint
                url = "https://openrouter.ai/api/v1/chat/completions"
                headers = {
                    "Authorization": f"Bearer {api_key}", 
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/DanKosgey/whatsapp-agent",
                    "X-Title": "WhatsApp AI Agent",
                    "Accept": "application/json"
                }
                logger.debug("OpenRouter API - Request headers prepared, URL: %s", url)

                client = self.http_client or httpx.AsyncClient(timeout=30.0)
                _created_here = not bool(self.http_client)
                try:
                    logger.info("OpenRouter API - Making POST request to %s", url)
                    r = await client.post(url, json=payload, headers=headers)
                    logger.debug("OpenRouter API - Response received, status: %d", r.status_code)
                    
                    # Log response details for better debugging
                    logger.debug("OpenRouter response status: %d", r.status_code)
                    if r.status_code != 200:
                        try:
                            error_data = r.json()
                            logger.warning("OpenRouter error response: %s", error_data)
                        except Exception:
                            logger.warning("OpenRouter error response (text): %s", r.text)
                    
                    # Handle different status codes with specific logic
                    if r.status_code == 404:
                        # Model not found - try a different model
                        logger.warning("OpenRouter 404 error: Model not found for %s", payload["model"])
                        # Try a different free model as fallback
                        fallback_models = [
                            "mistralai/mistral-7b-instruct:free",
                            "google/gemma-7b-it:free",
                            "microsoft/phi-3-mini-128k-instruct:free"
                        ]
                        # Cycle through fallback models
                        current_model = payload["model"]
                        try:
                            current_index = fallback_models.index(current_model)
                            payload["model"] = fallback_models[(current_index + 1) % len(fallback_models)]
                        except ValueError:
                            # Current model not in fallback list, use first fallback model
                            payload["model"] = fallback_models[0]
                        logger.info("Trying fallback model: %s", payload["model"])
                        
                        # Wait before retry with exponential backoff
                        if attempt < max_retries - 1:
                            wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                            logger.warning("OpenRouter 404 error (attempt %d/%d), trying different model in %ds", 
                                         attempt + 1, max_retries, wait_time)
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            raise RuntimeError(f"openrouter_model_not_found: Model {current_model} not found") from None
                    
                    elif r.status_code == 502:
                        # Bad gateway - temporary server issue
                        if attempt < max_retries - 1:
                            wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                            logger.warning("OpenRouter 502 error (attempt %d/%d), retrying in %ds", 
                                         attempt + 1, max_retries, wait_time)
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            raise RuntimeError("openrouter_bad_gateway: Server temporarily unavailable") from None
                    
                    elif r.status_code >= 500:
                        # Other server errors - retry with exponential backoff
                        if attempt < max_retries - 1:
                            wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                            logger.warning("OpenRouter server error %d (attempt %d/%d), retrying in %ds", 
                                         r.status_code, attempt + 1, max_retries, wait_time)
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            raise RuntimeError(f"openrouter_server_error: Server error {r.status_code}") from None
                    
                    elif r.status_code == 429:
                        # Rate limit error - mark key as temporarily unavailable
                        logger.warning("OpenRouter rate limit hit")
                        self.key_manager.mark_openrouter_key_unavailable(api_key, 120)  # 2 minutes
                        raise RuntimeError("openrouter_rate_limit_error: Rate limit exceeded") from None
                    
                    # Raise for other HTTP errors
                    r.raise_for_status()
                    data = r.json()
                    logger.debug("OpenRouter API - Response parsed successfully")
                finally:
                    if _created_here:
                        await client.aclose()

                if isinstance(data, dict):
                    choices = data.get("choices", [])
                    if choices and len(choices) > 0:
                        message = choices[0].get("message", {})
                        if message and "content" in message:
                            response_text = message["content"]
                            logger.info("OpenRouter API - Response processed successfully, text length: %d", len(response_text))
                            return {"text": response_text, "provider": "openrouter"}

                logger.warning("Unexpected OpenRouter response format (keys=%s)", list(data.keys()) if isinstance(data, dict) else type(data))
                return {"text": "I apologize, but I received an unexpected response format. Please try again.", "provider": "openrouter"}

            except Exception as e:
                # Handle httpx errors if httpx is available
                is_connection_error = False
                is_server_error = False
                is_rate_limit_error = False
                is_bad_gateway = False
                is_not_found = False
                
                if HTTPX_AVAILABLE:
                    try:
                        # Try to check if it's a connection error
                        is_connection_error = 'ConnectError' in str(type(e)) or 'ConnectTimeout' in str(type(e))
                        # Check for server errors (5xx)
                        is_server_error = '502' in str(e) or '503' in str(e) or '504' in str(e)
                        is_bad_gateway = '502' in str(e)
                        # Check for rate limit errors (429)
                        is_rate_limit_error = '429' in str(e)
                        # Check for not found errors (404)
                        is_not_found = '404' in str(e)
                        # Also check if it's an HTTPStatusError with status code 404
                        try:
                            response = getattr(e, 'response', None)
                            if response is not None:
                                status_code = getattr(response, 'status_code', None)
                                if status_code == 404:
                                    is_not_found = True
                        except Exception:
                            pass
                    except Exception:
                        # If we can't check, assume it's not a connection error
                        is_connection_error = False
                        is_server_error = False
                        is_rate_limit_error = False
                        is_bad_gateway = False
                        is_not_found = False
                
                # Log detailed error information
                logger.error("OpenRouter API call failed (attempt %d/%d): %s", attempt + 1, max_retries, str(e), exc_info=True)
                
                # If it's a rate limit error, implement exponential backoff
                if is_rate_limit_error:
                    # Calculate exponential backoff with jitter
                    wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                    logger.warning("OpenRouter rate limit hit (attempt %d/%d), waiting %ds", 
                                 attempt + 1, max_retries, wait_time)
                    if attempt < max_retries - 1:
                        await asyncio.sleep(wait_time)
                        continue
                    logger.exception("OpenRouter rate limit error after retries")
                    # Mark key as temporarily unavailable for longer period
                    self.key_manager.mark_openrouter_key_unavailable(api_key, 120)  # 2 minutes
                    raise RuntimeError(f"openrouter_rate_limit_error: {str(e)}") from e
                # If it's a 404 error, log details and try a different model as fallback
                elif is_not_found and attempt < max_retries - 1:
                    logger.warning("OpenRouter 404 error (attempt %d/%d), trying different model", 
                                 attempt + 1, max_retries)
                    # Try a different free model
                    fallback_models = [
                        "mistralai/mistral-7b-instruct:free",
                        "google/gemma-7b-it:free",
                        "microsoft/phi-3-mini-128k-instruct:free"
                    ]
                    # Cycle through fallback models
                    current_model = payload["model"]
                    try:
                        current_index = fallback_models.index(current_model)
                        payload["model"] = fallback_models[(current_index + 1) % len(fallback_models)]
                    except ValueError:
                        # Current model not in fallback list, use first fallback model
                        payload["model"] = fallback_models[0]
                    wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                    await asyncio.sleep(wait_time)
                    continue
                # If it's a bad gateway error, try a different model as fallback
                elif is_bad_gateway and attempt < max_retries - 1:
                    logger.warning("OpenRouter bad gateway error (attempt %d/%d), trying different model", 
                                 attempt + 1, max_retries)
                    # Try a different free model
                    fallback_models = [
                        "mistralai/mistral-7b-instruct:free",
                        "google/gemma-7b-it:free",
                        "microsoft/phi-3-mini-128k-instruct:free"
                    ]
                    # Cycle through fallback models
                    current_model = payload["model"]
                    try:
                        current_index = fallback_models.index(current_model)
                        payload["model"] = fallback_models[(current_index + 1) % len(fallback_models)]
                    except ValueError:
                        # Current model not in fallback list, use first fallback model
                        payload["model"] = fallback_models[0]
                    wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                    await asyncio.sleep(wait_time)
                    continue
                elif is_server_error and attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                    logger.warning("OpenRouter server error (attempt %d/%d), retrying in %ds", 
                                 attempt + 1, max_retries, wait_time)
                    await asyncio.sleep(wait_time)
                    continue
                elif is_connection_error:
                    last_error = e
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt) + (attempt * 0.1)  # Add jitter
                        logger.warning("OpenRouter connection failed (attempt %d/%d), retrying in %ds", 
                                     attempt + 1, max_retries, wait_time)
                        await asyncio.sleep(wait_time)
                        continue
                    logger.exception("OpenRouter final retry failed")
                    raise RuntimeError(f"openrouter_connection_error: {str(e)}") from e
                else:
                    logger.exception("OpenRouter request failed")
                    # For other errors, mark key as temporarily unavailable for a short period
                    if attempt >= max_retries - 1:
                        self.key_manager.mark_openrouter_key_unavailable(api_key, 30)  # 30 seconds
                    raise RuntimeError(f"openrouter_error: {str(e)}") from e

        if last_error:
            raise last_error
        return {"text": "OpenRouter response could not be processed", "provider": "openrouter"}

    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Generate a response using Gemini keys with rotation; fall back to OpenRouter if enabled.

        Returns a dict: {"text": str, "provider": "gemini"|"openrouter"}
        """
        logger.info("AI Generate - Starting generation request, prompt length: %d", len(prompt))
        now = datetime.now().astimezone()
        if self.next_retry_time and now < self.next_retry_time:
            logger.info("AI temporarily unavailable until %s", self.next_retry_time.isoformat())
            # Reset the retry time to allow trying again if enough time has passed
            if (now - self.next_retry_time).total_seconds() > 300:  # 5 minutes
                self.next_retry_time = None
                self.ai_available = True
            else:
                raise AIProviderExhausted("ai_unavailable")

        # If DEV_SMOKE env var is set OR the handler was constructed with a DEBUG config,
        # return a deterministic local reply immediately (convenience for local dev/testing).
        try:
            from server.config import config as _cfg2
        except Exception:
            _cfg2 = None
        dev_env = getattr(_cfg2, "DEV_SMOKE", False) if _cfg2 is not None else (os.getenv("DEV_SMOKE", "0").lower() in ("1", "true", "yes"))
        dev = dev_env or getattr(self, "_force_dev", False)
        if dev:
            logger.info("DEV_SMOKE active: returning deterministic reply")
            dev_response = {"text": f"[DEV_SMOKE reply to: {prompt}]", "provider": "dev"}
            logger.debug("DEV_SMOKE response: %s", dev_response)
            return dev_response

        # Get tools if available
        tools = None
        if self.tools:
            try:
                tools = self.tools.get_all_tool_schemas()
                logger.debug("AI Generate - Retrieved %d tool schemas", len(tools) if tools else 0)
            except Exception as e:
                logger.warning(f"Failed to get tool schemas: {e}")

        # If no keys configured and no fallback available, raise clear error
        if not self.gemini_keys and not (self.enable_openrouter_fallback and self.openrouter_keys):
            # Check if we're in DEV_SMOKE mode
            if dev:
                dev_response = {"text": f"[DEV_SMOKE reply to: {prompt}]", "provider": "dev"}
                logger.debug("DEV_SMOKE response (no keys): %s", dev_response)
                return dev_response
            
            # Provide a more informative error message
            error_msg = "No AI API keys configured. "
            if len(self.gemini_keys) == 0 and len(self.openrouter_keys) == 0:
                error_msg += "Please add API keys using the manage_api_keys.py script or set environment variables."
            elif len(self.gemini_keys) == 0:
                error_msg += "No Gemini API keys found. Please add Gemini API keys."
            else:
                error_msg += "OpenRouter fallback is disabled. Please enable it or add more API keys."
            
            logger.error(error_msg)
            raise RuntimeError(error_msg)

        # Try Gemini keys first
        logger.info("AI Generate - Attempting Gemini keys (count: %d)", len(self.gemini_keys))
        tried_gemini = 0
        gemini_count = len(self.gemini_keys)
        consecutive_failures = 0
        # Reduce the max consecutive failures to make the circuit breaker less aggressive
        max_consecutive_failures = 10  # Increased from 5 to 10 for even more resilience
        
        while tried_gemini < max(1, gemini_count) and consecutive_failures < max_consecutive_failures:
            key = self._next_gemini_key()
            if not key:
                logger.warning("AI Generate - No available Gemini keys found")
                break
            try:
                logger.debug("Attempting Gemini with key index %d", self._gemini_idx - 1)
                resp = await self._call_gemini(prompt, key, tools)
                self.ai_available = True
                consecutive_failures = 0  # Reset consecutive failures on success
                logger.info("AI Generate - Gemini call successful with provider: %s", resp.get("provider", "unknown"))
                
                # Handle function calls if present
                if isinstance(resp, dict) and "function_calls" in resp:
                    # Process function calls and get final response
                    logger.info("AI Generate - Processing function calls from Gemini response")
                    final_response = await self._process_function_calls(prompt, resp)
                    # Record key usage
                    key_obj = next((k for k in self.key_manager.gemini_keys if k.key == key), None)
                    if key_obj:
                        self.key_manager.record_use(key_obj)
                    logger.debug("AI Generate - Function calls processed, final response: %s", 
                                {k: v for k, v in final_response.items() if k != 'raw_response'})
                    return final_response
                
                # Record key usage
                key_obj = next((k for k in self.key_manager.gemini_keys if k.key == key), None)
                if key_obj:
                    self.key_manager.record_use(key_obj)
                logger.debug("AI Generate - Gemini response: %s", 
                            {k: v for k, v in resp.items() if k != 'raw_response'})
                return resp
            except Exception as e:
                error_msg = str(e).lower()
                logger.warning("Gemini key failed: %s", str(e))
                consecutive_failures += 1
                
                # If it's a quota error, mark the key as temporarily unavailable
                if "quota exceeded" in error_msg or "429" in error_msg:
                    # Mark the key as temporarily unavailable
                    self.key_manager.mark_gemini_key_unavailable(key, 120)  # Unavailable for 2 minutes
                    logger.warning("Gemini key marked as unavailable due to quota limits")
                    # Don't increment tried_gemini for quota errors, try next key
                    continue
                else:
                    # For other errors, increment the counter
                    tried_gemini += 1
                    logger.debug("Gemini key failure counted, tried_gemini: %d", tried_gemini)

        # If we get here, Gemini keys failed or were not present
        # Use a shorter cooldown period and make it less aggressive
        if consecutive_failures >= max_consecutive_failures:
            logger.warning("Too many consecutive Gemini failures, temporarily disabling AI")
            self.ai_available = False
            # Use a shorter cooldown period (30 seconds instead of 1 minute)
            self.next_retry_time = datetime.now().astimezone() + timedelta(seconds=30)
        else:
            logger.info("All Gemini keys exhausted or failed")
            # set a shorter retry cooldown
            self.ai_available = False
            self.next_retry_time = datetime.now().astimezone() + timedelta(seconds=15)  # Reduced from 30 to 15 seconds

        if self.enable_openrouter_fallback and self.openrouter_keys:
            logger.info("AI Generate - Gemini failed, attempting OpenRouter fallback (keys: %d)", len(self.openrouter_keys))
            # Try OpenRouter keys
            tried_or = 0
            or_count = len(self.openrouter_keys)
            consecutive_failures = 0
            # Reduce the max consecutive failures to make the circuit breaker less aggressive
            max_consecutive_failures = 10  # Increased from 5 to 10 for even more resilience
            
            while tried_or < max(1, or_count) and consecutive_failures < max_consecutive_failures:
                or_key = self._next_openrouter_key()
                if not or_key:
                    logger.warning("AI Generate - No available OpenRouter keys found")
                    break
                try:
                    logger.debug("Attempting OpenRouter with key index %d", self._openrouter_idx - 1)
                    resp = await self._call_openrouter(prompt, or_key)
                    # mark AI available since fallback succeeded
                    self.ai_available = True
                    self.next_retry_time = None
                    consecutive_failures = 0  # Reset consecutive failures on success
                    logger.info("AI Generate - OpenRouter call successful with provider: %s", resp.get("provider", "unknown"))
                    
                    # Record key usage
                    key_obj = next((k for k in self.key_manager.openrouter_keys if k.key == or_key), None)
                    if key_obj:
                        self.key_manager.record_use(key_obj)
                    logger.debug("AI Generate - OpenRouter response: %s", resp)
                    return resp
                except Exception as e:
                    error_msg = str(e).lower()
                    logger.warning("OpenRouter key failed: %s", str(e))
                    consecutive_failures += 1
                    
                    # If it's a rate limit error, mark the key as temporarily unavailable
                    if "rate limit" in error_msg or "429" in error_msg:
                        # Mark the key as temporarily unavailable
                        self.key_manager.mark_openrouter_key_unavailable(or_key, 120)  # Unavailable for 2 minutes
                        logger.warning("OpenRouter key marked as unavailable due to rate limits")
                        # Don't increment tried_or for rate limit errors, try next key
                        continue
                    else:
                        # For other errors, increment the counter
                        tried_or += 1
                        logger.debug("OpenRouter key failure counted, tried_or: %d", tried_or)

        # Nothing worked
        logger.error("No AI providers available after rotation")
        # Reset AI availability after a shorter cooldown period
        self.next_retry_time = datetime.now().astimezone() + timedelta(seconds=30)  # Reduced from 2 minutes to 30 seconds
        raise AIProviderExhausted("no_providers_available")
